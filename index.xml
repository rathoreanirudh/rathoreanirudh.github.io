<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anirudh Rathore</title>
    <link>https://rathoreanirudh.github.io/</link>
    <description>Recent content on Anirudh Rathore</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Jan 2020 19:37:54 -0700</lastBuildDate>
    
	<atom:link href="https://rathoreanirudh.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Eraser</title>
      <link>https://rathoreanirudh.github.io/posts/2020/01/eraser/</link>
      <pubDate>Sat, 25 Jan 2020 19:37:54 -0700</pubDate>
      
      <guid>https://rathoreanirudh.github.io/posts/2020/01/eraser/</guid>
      <description>ERASER stands for Evaluating Rationales And Simple English Reasoning benchmark to advance research on interpretable models in NLP. ERASER focuses specifically on rationales which are snippets of text from a source document that support a particular categorization. Moreover, another consideration for rationales is that they are comprehensive rather than simply sufficient.
Metrics for the datasets  Those based on exact match Ranking metrics that provide a measure of model&#39;s ability to discriminate between evidence and non evidence tokens  </description>
    </item>
    
    <item>
      <title>Interpretable Machine Learning Murdoch</title>
      <link>https://rathoreanirudh.github.io/posts/2020/01/interpretable-machine-learning-murdoch/</link>
      <pubDate>Tue, 21 Jan 2020 16:36:05 -0700</pubDate>
      
      <guid>https://rathoreanirudh.github.io/posts/2020/01/interpretable-machine-learning-murdoch/</guid>
      <description>This is the second reading of my week. Actually my first in-depth reading. I plan to do research in interpretable machine learning and trying to understand what causes bias to creep into the machine learning models.
In this paper by Murdoch et al., they start by definining interpretability and what it means at different stages of the data science life cycle. According to the paper, in the absence of a well formed definition of interpretability</description>
    </item>
    
    <item>
      <title>Human Decisions Machine Predictions Reading</title>
      <link>https://rathoreanirudh.github.io/posts/2020/01/human-decisions-machine-predictions-reading/</link>
      <pubDate>Sun, 19 Jan 2020 18:16:21 -0700</pubDate>
      
      <guid>https://rathoreanirudh.github.io/posts/2020/01/human-decisions-machine-predictions-reading/</guid>
      <description>This post is for my human centered machine learning class. We are required to read a paper every week and write a summary for it. I am trying to make a habit of writing this blog describing the papers I read.</description>
    </item>
    
  </channel>
</rss>