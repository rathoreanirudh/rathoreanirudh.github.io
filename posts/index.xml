<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Anirudh Rathore</title>
        <link>https://rathoreanirudh.github.io/posts/</link>
        <description>Recent content in Posts on Anirudh Rathore</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 28 Jan 2020 16:27:54 -0700</lastBuildDate>
        <atom:link href="https://rathoreanirudh.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Lime</title>
            <link>https://rathoreanirudh.github.io/posts/2020/01/lime/</link>
            <pubDate>Tue, 28 Jan 2020 16:27:54 -0700</pubDate>
            
            <guid>https://rathoreanirudh.github.io/posts/2020/01/lime/</guid>
            <description></description>
            <content type="html"><![CDATA[]]></content>
        </item>
        
        <item>
            <title>Eraser</title>
            <link>https://rathoreanirudh.github.io/posts/2020/01/eraser/</link>
            <pubDate>Sat, 25 Jan 2020 19:37:54 -0700</pubDate>
            
            <guid>https://rathoreanirudh.github.io/posts/2020/01/eraser/</guid>
            <description>ERASER stands for Evaluating Rationales And Simple English Reasoning benchmark to advance research on interpretable models in NLP. ERASER focuses specifically on rationales which are snippets of text from a source document that support a particular categorization. Moreover, another consideration for rationales is that they are comprehensive rather than simply sufficient.
Metrics for evaluation  Assess the degree to which the rationales extacted by a model agree with those highlighted by human annotators.</description>
            <content type="html"><![CDATA[<p>ERASER stands for Evaluating Rationales And Simple English Reasoning benchmark to advance research on interpretable models in NLP. ERASER focuses specifically on rationales which are snippets of text from a source document that support a particular categorization.
Moreover, another consideration for rationales is that they are comprehensive rather than simply sufficient.</p>
<h2 id="metrics-for-evaluation">Metrics for evaluation</h2>
<ul>
<li>Assess the degree to which the rationales extacted by a model agree with those highlighted by human annotators.</li>
<li>To measure the exact and a partial match by adopting metrics from NER and object detection.</li>
<li>Ranking metrics that provide a measure of model's ability to discriminate between evidence and non evidence tokens</li>
</ul>
<h2 id="current-datasets-in-eraser">Current datasets in ERASER</h2>
<ul>
<li>Evidence Inference</li>
<li>BoolQ</li>
<li>Movie Review</li>
<li>FEVER</li>
<li>MultiRC</li>
<li>CoS-E</li>
<li>e-SNLI</li>
</ul>
<h3 id="further-research-inspiration">Further Research inspiration</h3>
<ul>
<li>How best to measure the quality of model explanations in the context of NLP</li>
<li>problem of designing metrics for evaluating rationales - especially for capturing faithfulness</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Interpretable Machine Learning Murdoch</title>
            <link>https://rathoreanirudh.github.io/posts/2020/01/interpretable-machine-learning-murdoch/</link>
            <pubDate>Tue, 21 Jan 2020 16:36:05 -0700</pubDate>
            
            <guid>https://rathoreanirudh.github.io/posts/2020/01/interpretable-machine-learning-murdoch/</guid>
            <description>This is the second reading of my week. Actually my first in-depth reading. I plan to do research in interpretable machine learning and trying to understand what causes bias to creep into the machine learning models.
In this paper by Murdoch et al., they start by definining interpretability and what it means at different stages of the data science life cycle. According to the paper, in the absence of a well formed definition of interpretability</description>
            <content type="html"><![CDATA[<p>This is the second reading of my week. Actually my first in-depth reading. I plan to do research in interpretable machine learning and trying to understand what causes bias to creep into the machine learning models.</p>
<p>In this paper by Murdoch et al., they start by definining interpretability and what it means at different stages of the data science life cycle. According to the paper, in the absence of a well formed definition of interpretability</p>
<ul>
<li>visualizations</li>
<li>natural language</li>
<li>mathematical equations
are labeled as interpretation</li>
</ul>
<p><img src="https://miro.medium.com/max/1278/1*tX9jrdkCZm3jPqG0rWXwOQ.png" alt="Example image"></p>
<h1 id="todo">TODO</h1>
]]></content>
        </item>
        
        <item>
            <title>Human Decisions Machine Predictions Reading</title>
            <link>https://rathoreanirudh.github.io/posts/2020/01/human-decisions-machine-predictions-reading/</link>
            <pubDate>Sun, 19 Jan 2020 18:16:21 -0700</pubDate>
            
            <guid>https://rathoreanirudh.github.io/posts/2020/01/human-decisions-machine-predictions-reading/</guid>
            <description>This post is for my human centered machine learning class. We are required to read a paper every week and write a summary for it. I am trying to make a habit of writing this blog describing the papers I read.</description>
            <content type="html"><![CDATA[<p>This post is for my human centered machine learning class. We are required to read a paper every week and write a summary for it. I am trying to make a habit of writing this blog describing the papers I read.</p>
]]></content>
        </item>
        
    </channel>
</rss>
